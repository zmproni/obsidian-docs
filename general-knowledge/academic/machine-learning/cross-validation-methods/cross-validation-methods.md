# Cross-validation Methods
Cross-validation is a technique where data is divided such that it is possible to use the various subsets of data to train and evaluate the performance of a model. Cross-validation allows users to assess the performance of a model, tune mode parameters, help decide which model performs best and even help train a model. There are various techniques to cross-validation and each have their benefits and downsides. The specific ones that will be discussed are:
- [**Hold-out validation**](hold-out-validation)
- [**K-fold cross validation**](k-fold-validation)
- [**Stratified K-fold cross validation**](stratified-k-fold-validation)
- [**Repeated Stratified K-fold cross validation**](repeated-stratified-k-fold-validation.md)
- [**Leave One Out Cross Validation (LOOCV)**](LOOCV.md)
There are more cross validation techniques, however we will only tackle the ones mentioned above (perhaps more will be added in the future).  

## Notes
[Link](https://www.canva.com/design/DAFXkt4S488/gsfnJYQrFMNi4_Q5NEFnCQ/edit) to the worspace used to create the graphics. 
