# F1 Score
F1 score refers to the adjusted accuracy based on precision and recall. It is described as the harmonic mean between the two. 
$$F1 = 2 {precision * recall \over precision + recall}$$
The F1 score is particularly useful when the cost of false positives and false negatives is balanced, such as in fraud detection, where it is important to identify as many fraud cases as possible while minimising the number of false alarms.

It's also important to note that F1 score is a better metric than accuracy when the classes are imbalanced.

In summary, the F1 score is a measure of a test's accuracy, it's the harmonic mean of precision and recall, it ranges between 0 and 1, where 1 is the best value. It's particularly useful when the cost of false positives and false negatives is balanced, and it's a better metric than accuracy when the classes are imbalanced.

F1 score is a better metric than accuracy when the classes are imbalanced, meaning that the number of observations in one class is much greater than the other. In such cases, accuracy can be misleading as a model that simply predicts the majority class will achieve high accuracy even though it is not making good predictions.